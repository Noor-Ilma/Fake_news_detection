{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c218c25a-2ca2-4be0-826b-f44d0bd43010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords # the for on in with these should be removed\n",
    "from nltk.stem.porter import PorterStemmer # played playing == play\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # played ==[0.0]\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79421613-d30a-4519-b0e3-462b4a606763",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b18f614-ab77-454d-a4ad-1b4741d33577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text;label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dark agenda behind globalism open border altma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>america poor still get shaft sami jamil jadall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>number accuser grow former miss finland accuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heroic prego advertisement replaces refresh we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>russia syria debbie reynolds thursday even bri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text;label\n",
       "0  dark agenda behind globalism open border altma...\n",
       "1  america poor still get shaft sami jamil jadall...\n",
       "2  number accuser grow former miss finland accuse...\n",
       "3  heroic prego advertisement replaces refresh we...\n",
       "4  russia syria debbie reynolds thursday even bri..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3518d145-5ada-40a5-bba9-936a5ce01e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#About the Dataset:\n",
    "\n",
    "#text: Contains the content of the news article (or a portion of it).\n",
    "#label: A binary label indicating whether the news article is real or fake.\n",
    "#1: Fake News\n",
    "#0: Real News\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "504ffdd6-53b1-4b62-8b0a-10bf376ff77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text;label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "news_df.isnull().sum() #to check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3da78e58-212a-4999-8dec-9e1e77a6ffd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16646, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape  #to get the rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3d2d89e-b263-4c69-8ea8-986a5f79c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.fillna(' ') #to fill a space in the null value places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d772b65f-c148-456b-9aa1-4842a594b5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text;label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13ab497f-26b3-4c44-b819-5f7d3fc509aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "news_df['content'] = news_df['text']\n",
    "#The error KeyError: 'text' means that the column name 'text' does not exist in your DataFrame.This likely happened because your dataset was not properly split into columns due to an incorrect delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23be25a8-e961-4e46-9e52-9bcb9767d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text;label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(news_df.columns)\n",
    "#check the actual column names in dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c2f0c2-dfb0-46f5-9be8-431e406a66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  dark agenda behind globalism open border altma...      0\n",
      "1  america poor still get shaft sami jamil jadall...      0\n",
      "2  number accuser grow former miss finland accuse...      0\n",
      "3  heroic prego advertisement replaces refresh we...      0\n",
      "4  russia syria debbie reynolds thursday even bri...      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv(\"train.csv\", delimiter=\";\")  # Use correct delimiter\n",
    "print(news_df.head())  # Check if columns are separated properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde8ca76-4e87-4d18-b14f-c0dab437552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16646, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape   #to get the rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49440536-7a54-4104-a3de-363b46059817",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.fillna(' ')  #to fill a space in the null value places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d97ab85-8fe7-4c45-83ca-340b6b4f60fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum() #to check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171b0f0e-bd5d-4f36-9128-ea58bf48e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK packages downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print(\"NLTK packages downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4dd073-28a3-4713-a82b-e317b299baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in c:\\users\\noori\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from swifter) (2.1.4)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from swifter) (5.9.0)\n",
      "Requirement already satisfied: dask>=2.10.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2023.11.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from swifter) (4.65.0)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (7.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\noori\\anaconda3\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.17.0)\n",
      "Requirement already satisfied: locket in c:\\users\\noori\\anaconda3\\lib\\site-packages (from partd>=1.2.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\noori\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b82e89-7bdf-4984-a492-db58ed6d6b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf5b5a9777a418483395a47ad1dd9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed and saved to 'cleaned_train.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import swifter  # Install using: pip install swifter\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset with proper delimiter handling\n",
    "news_df = pd.read_csv(\"train.csv\", delimiter=\";\", engine=\"python\")\n",
    "\n",
    "# Check and fix column names if needed\n",
    "if \"text;label\" in news_df.columns:\n",
    "    news_df[['text', 'label']] = news_df['text;label'].str.split(';', expand=True)\n",
    "    news_df.drop(columns=['text;label'], inplace=True)\n",
    "\n",
    "# Verify if 'text' column exists now\n",
    "if 'text' not in news_df.columns:\n",
    "    raise KeyError(\"The 'text' column is missing after processing. Check the dataset format.\")\n",
    "\n",
    "# Load stopwords once (for efficiency)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]  # Use preloaded stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing using swifter (parallel processing)\n",
    "news_df['text'] = news_df['text'][:1000].swifter.apply(preprocess_text)  # Process first 1000 rows for testing\n",
    "\n",
    "# Save cleaned data\n",
    "news_df.to_csv(\"cleaned_train.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessing completed and saved to 'cleaned_train.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ff1abe-eb25-43bc-9cd6-8386d5a5ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the data & label\n",
    "X = news_df.drop('label',axis=1)\n",
    "y = news_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908bcd9e-bec9-48f0-b74a-480abc3b2706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0      dark agenda behind globalism open border altma...\n",
      "1      america poor still get shaft sami jamil jadall...\n",
      "2      number accuser grow former miss finland accuse...\n",
      "3      heroic prego advertisement replaces refresh we...\n",
      "4      russia syria debbie reynolds thursday even bri...\n",
      "...                                                  ...\n",
      "16641                                                NaN\n",
      "16642                                                NaN\n",
      "16643                                                NaN\n",
      "16644                                                NaN\n",
      "16645                                                NaN\n",
      "\n",
      "[16646 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ad6f61-b463-4c95-ac7b-1d37a25fad18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStemming:\\nStemming is the process of reducing a word to its Root word\\n\\nexample: hung hanged hanging ======hang\\n\\nSteps:\\nlower case\\nsplitting\\nremoving stopwords\\nstemming\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stemming:\n",
    "Stemming is the process of reducing a word to its Root word\n",
    "\n",
    "example: hung hanged hanging ======hang\n",
    "\n",
    "Steps:\n",
    "lower case\n",
    "splitting\n",
    "removing stopwords\n",
    "stemming\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f138d6-89ff-462c-bd2b-4e069eb2027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed and saved to 'cleaned_train.csv'\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmer and stopwords once\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define stemming function with error handling\n",
    "def stemming(text):\n",
    "    if not isinstance(text, str):  # Ensure input is a string\n",
    "        return \"\"  # Return empty string for non-string values\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()  # Tokenize\n",
    "    words = [ps.stem(word) for word in words if word not in stop_words]  # Stemming + remove stopwords\n",
    "    return ' '.join(words)  # Rejoin words\n",
    "\n",
    "# Load dataset\n",
    "news_df = pd.read_csv(\"train.csv\", delimiter=\";\", engine=\"python\")\n",
    "\n",
    "# Check and fix column names if needed\n",
    "if \"text;label\" in news_df.columns:\n",
    "    news_df[['text', 'label']] = news_df['text;label'].str.split(';', expand=True)\n",
    "    news_df.drop(columns=['text;label'], inplace=True)\n",
    "\n",
    "# Apply stemming to the 'text' column\n",
    "news_df['text'] = news_df['text'].astype(str).apply(stemming)\n",
    "\n",
    "# Save cleaned data\n",
    "news_df.to_csv(\"cleaned_train.csv\", index=False)\n",
    "print(\"Preprocessing completed and saved to 'cleaned_train.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0640987f-a7ce-4734-9b6d-2ff376bc2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['text'] = news_df['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06688ff5-1e2a-4765-9604-49dd326c73f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        dark agenda behind global open border altmarke...\n",
       "1        america poor still get shaft sami jamil jadall...\n",
       "2        number accu grow former miss finland accu trum...\n",
       "3        heroic prego adverti replac refresh webpag pre...\n",
       "4        russia syria debbi reynold thursday even brief...\n",
       "                               ...                        \n",
       "16641    comment cost selfdriv taxi realli sixtyseven c...\n",
       "16642    interest dutert get billion china get money ja...\n",
       "16643    forget encyclopaedia get pokiespedia home win ...\n",
       "16644    u elect race huma abedin connect minut video n...\n",
       "16645    shame obama legaci white man beat viciou vote ...\n",
       "Name: text, Length: 16646, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e23cce02-145e-4f85-8937-f5f755c11866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the data and label\n",
    "# Extract features and labels\n",
    "X = news_df['text'].values  # Use 'text' instead of 'content'\n",
    "y = news_df['label'].values  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df3f8b3a-01a2-47c4-ab47-58836c0a927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the textual data to numerical data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "638f8a3f-9839-43e5-a42a-9d2d2dd03307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34015)\t0.03284309433004357\n",
      "  (0, 16687)\t0.00763985160811272\n",
      "  (0, 22809)\t0.02379168166533704\n",
      "  (0, 15811)\t0.015835621663781812\n",
      "  (0, 47979)\t0.010269070425615813\n",
      "  (0, 93127)\t0.011356457762665181\n",
      "  (0, 94344)\t0.016309199992176608\n",
      "  (0, 38458)\t0.02755487311399001\n",
      "  (0, 17517)\t0.01754530891487748\n",
      "  (0, 65719)\t0.011165687668851725\n",
      "  (0, 47431)\t0.014574144978956356\n",
      "  (0, 81568)\t0.01202024373179219\n",
      "  (0, 46282)\t0.008255268779848687\n",
      "  (0, 72875)\t0.043559541240720465\n",
      "  (0, 88922)\t0.02834437949254646\n",
      "  (0, 18172)\t0.012451129209672299\n",
      "  (0, 21438)\t0.02922817287661162\n",
      "  (0, 86724)\t0.023957795592866107\n",
      "  (0, 72825)\t0.02484021985754806\n",
      "  (0, 20573)\t0.014274461994284891\n",
      "  (0, 93063)\t0.016158813253991402\n",
      "  (0, 26453)\t0.010703270880873705\n",
      "  (0, 1192)\t0.014675049654489156\n",
      "  (0, 6363)\t0.008884110893008645\n",
      "  (0, 32508)\t0.007088798489671592\n",
      "  :\t:\n",
      "  (16645, 95818)\t0.055347865957992154\n",
      "  (16645, 22710)\t0.04171271971445988\n",
      "  (16645, 87026)\t0.17802894211521525\n",
      "  (16645, 77378)\t0.05983198029917312\n",
      "  (16645, 77410)\t0.03988466854414771\n",
      "  (16645, 24721)\t0.10187152942449705\n",
      "  (16645, 93197)\t0.18135130003160702\n",
      "  (16645, 9325)\t0.0570506114630283\n",
      "  (16645, 2978)\t0.036893120145946194\n",
      "  (16645, 24733)\t0.04077783888076775\n",
      "  (16645, 2963)\t0.046200413430518006\n",
      "  (16645, 50041)\t0.2975814708640752\n",
      "  (16645, 11228)\t0.04753750784674489\n",
      "  (16645, 75687)\t0.04684900273746996\n",
      "  (16645, 48668)\t0.060054589569483265\n",
      "  (16645, 89551)\t0.06777384660269142\n",
      "  (16645, 76061)\t0.26802996706995325\n",
      "  (16645, 94191)\t0.03507779630140139\n",
      "  (16645, 51871)\t0.043227828482766316\n",
      "  (16645, 74930)\t0.03468482094232043\n",
      "  (16645, 31744)\t0.031836504824655054\n",
      "  (16645, 35609)\t0.04979720979281333\n",
      "  (16645, 26598)\t0.05646239815968688\n",
      "  (16645, 94247)\t0.038941409737166144\n",
      "  (16645, 63084)\t0.02935094526259933\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "280e63aa-870e-4ffd-a14a-08300df14b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset to training & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e57a741b-92bb-440a-9b13-eb8074c7327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9def4091-b74e-4b5d-9575-eebb31e70227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13316, 96715)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75455c-4722-4253-86ac-38b3d3745bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff255176-eceb-4a19-aa21-ead97a499a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6ab66b6-2799-41f1-85a5-21a9f2771e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731901471913488\n"
     ]
    }
   ],
   "source": [
    "# on training set\n",
    "train_y_pred = model.predict(X_train)\n",
    "print(accuracy_score(train_y_pred,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84fc1074-b0ef-45c1-bbd5-dbb72aa9acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453453453453453\n"
     ]
    }
   ],
   "source": [
    "# on testing set\n",
    "testing_y_pred = model.predict(X_test)\n",
    "print(accuracy_score(testing_y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b2189c9-846a-4653-9170-7ff04bf2b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24c3d1e9-97b7-45d1-9128-c4080b8e9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = X_test[10]\n",
    "prediction = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d28da98-31e0-435b-b4d4-85048bb511a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The News Is Real\n"
     ]
    }
   ],
   "source": [
    "if prediction[0] == 0:\n",
    "    print('The News Is Real')\n",
    "else:\n",
    "    print('The News is Fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5426d32-bd3a-4b2f-805a-9821419dd951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'america poor still get shaft sami jamil jadallah novemb day ugli distast presidenti elect u histori see candid presid u presid elect grope email make u laugh stock world saw campaign littl anyth prioriti realli matter american peopl candid talk bring back job industri manufactur job know well liar never bring back kind industri manufactur job give middleclass american sen digniti secur two candid best cater rich power peopl money billionair alway shape nation agenda polit billion pour year nation elect differ previou nationalpresidenti elect close billion inject invest elect return donor never voter america poor black hispan white well neglect long time get shaft everywh everyon especi congress empow corpor america screw poor everi turn everi way congress legisl give incent corpor locat manufactur job oversea allow bank financ institut especi credit card compani give shaft poor futur hold next gener surpri great countri stagger number poor million popul prospect american part categori least one year age year poor unemploy live system million poor work hard hard one two job make end meet hispan black like among categori asian white woman like men fall categori work poor white male less high school educ spend least one year categori hard think countri plenti million peopl earn less year million deep poverti earn less year presidenti candid cater rich power seek support candid talk poorpay job american avail job pay less annual talk bank financ institut loot ten million american pension fund financ crisi fact congress allow corpor restructur benefit wall street investor cheat worker pension health care cour republican talk elimin entitl poor forget neglect kind entitl u militari get kind entitl corpor rich execut get part deal work member congress entitl tripl hardwork poor american get true famili get cash assist black hispan lead believ thu cau outcri end welfar system note entitl go elderli poor retir worker social secur medicar amount billion let u look system sponsor nurtur congress screw american everi day let u take exampl interest rate charg credit card case poor one get behind credit card payment interest rate charg almost twice much averag rate moreov one get behind one payment one credit card credit card compani hike rate thu poor pay high price cost highinterest rate charg likelihood default one payment credit rate chang forc poor pay high interest car loan mortgag loan truli rip fact compani like payday loan car titl compani consid bank receiv fund less charg borrow even unconscion poor also get shaft health care compani especi supplement insur fact poor like suffer certain chronic disea obe high blood pressur diabet rate much high averag add way school fund sinc nation tax plan fund educ fund come properti tax poor neighborhood incom support school system polic fire depart much less affluent commun child get less qualiti educ like among poor live govern subsidi like neighborhood high crime rate higher percentag incarc govern save support educ pay doubl cost crime incarc simpli make sen societi leav two incid person experi supplement health insur mean u age must purcha supplement health insur complement medicar purcha polici unitedhealth aarp pay month almost year discov ceo unitedhealth earn year salari make simpl calcul shock realiz pay year monthli premium pay salari one year cour drop unitedhealth exploit disadvantag commun privat compani offer auto dealer smart loyalti card dealer add card special packag oil chang discount mainten extend warranti consum benefit abl keep track vehicl servic mainten record smart chip card patent thu save car owner get rip duplic unwarr repair mani dealer especi poor neighborhood charg poor interest rate exceed car loan forc poor guy buy addit warranti charg thousand dollar servic dealer know custom would never need drop dealer network know mayb one day congress presidenti candid repr rich poor middl class offer solut get peopl work welfar subsidi american peopl lazi need opportun trump clinton bring opportun america problem presid congress chang must take place'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa332b-95b0-47f0-9e55-1170d2c48bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOTHER METHOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab970925-dd88-4013-a2d1-ae4c37c79502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
